# BadaBoomBooks Environment Configuration
# Copy this file to .env and fill in your actual values

# === LLM Configuration ===
# Enable LLM-based candidate selection with --llm-select flag

# LLM API Key (required for LLM features)
# For OpenAI: sk-...
# For Anthropic: sk-ant-...
# For local models (LM Studio/Ollama): any dummy value like "local"
LLM_API_KEY=

# LLM Model to use (optional, default: gpt-3.5-turbo)
# OpenAI examples: gpt-3.5-turbo, gpt-4, gpt-4-turbo
# Anthropic examples: claude-3-sonnet-20240229, claude-3-opus-20240229
# Local model examples: any model name loaded in LM Studio/Ollama
LLM_MODEL=gpt-3.5-turbo

# Base URL for local models (optional)
# For LM Studio (default): http://localhost:1234/v1
# For Ollama: http://localhost:11434/v1
# Leave empty for commercial APIs (OpenAI, Anthropic)
OPENAI_BASE_URL=

# Maximum tokens for LLM responses (optional, default: 4096)
# Increase if LLM responses are being truncated
# Decrease to reduce token usage and cost
LLM_MAX_TOKENS=4096

# === Usage Examples ===

# Example 1: OpenAI GPT-3.5 Turbo
# LLM_API_KEY=sk-proj-abc123...
# LLM_MODEL=gpt-3.5-turbo
# OPENAI_BASE_URL=

# Example 2: OpenAI GPT-4
# LLM_API_KEY=sk-proj-abc123...
# LLM_MODEL=gpt-4
# OPENAI_BASE_URL=

# Example 3: Anthropic Claude
# LLM_API_KEY=sk-ant-abc123...
# LLM_MODEL=claude-3-sonnet-20240229
# OPENAI_BASE_URL=

# Example 4: Local LM Studio
# LLM_API_KEY=lm-studio
# LLM_MODEL=local-model
# OPENAI_BASE_URL=http://localhost:1234/v1

# Example 5: Local Ollama
# LLM_API_KEY=ollama
# LLM_MODEL=llama2
# OPENAI_BASE_URL=http://localhost:11434/v1
